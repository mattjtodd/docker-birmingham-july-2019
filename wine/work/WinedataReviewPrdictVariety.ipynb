{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This was a smaall project to while away Xmas last year (2017).\n",
    "\n",
    "### Goals\n",
    "\n",
    "* Attenmpt to predict the variety of wine from the published wine reviews\n",
    "* Build and run a Jupyter Notebook in Docker to run Toree & Spark in a Kernel\n",
    "* Use Spark ML to train a model\n",
    "* Explore ways of creating a good science toolset with a mind to creating a completely reproducible experiment regardless of localtion\n",
    "* Render some nice charts\n",
    "* Enjoy some nice wine whilst doing so!\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "Containerisation is a great way to package and share a highly reproduicble environment for running Jupyter Notesbooks and their runtime kernels.  The ability to access a hosts physical filesystem through bind-mounts also makes it possible to seperate this from the runtime process further reducing the coupling increasing the reusability of the images.  As the files are located physically outside of the container, they can be maipulated locally, be comitted to git repos and shared with others.\n",
    "\n",
    "### Process\n",
    "\n",
    "\n",
    "### LInks\n",
    "\n",
    "* [Jupyter Notebooks](https://jupyter.org/)\n",
    "* [Docker Stack For Jupyter](https://jupyter-docker-stacks.readthedocs.io/en/latest/)\n",
    "* [Apache Toree](https://toree.apache.org/)\n",
    "* [Apache SparlML](https://spark.apache.org/mllib/)\n",
    "* [Kaggle Wine Reviews Dataset](https://www.kaggle.com/zynicide/wine-reviews)\n",
    "* [Bruel Charts](https://github.com/Brunel-Visualization/Brunel/wiki)\n",
    "\n",
    "### Extensions\n",
    "\n",
    "* Performn data analysis to check spread of data\n",
    "* Verify the current feature extraction, what are the most imprtant features for training the model\n",
    "* Use other feature extraction methods\n",
    "* Use other model algos\n",
    "* COnnect to a real Spark cluster running in the cloud AWS / Azure etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "This section sets up the rendering library for data.  This uses a \"magic\" to download and install the [Brunel](https://github.com/Brunel-Visualization/Brunel/wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%AddJar -magic http://brunelvis.org/jar/spark-kernel-brunel-all-2.2.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the _local_ spark context and imports the explicits for the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read gzipped raw json document\n",
    "\n",
    "This will handily gunzip the file as a stream and attempt to infer the scehama from the data.  The will load the data into spark DataFrames through which table based and queries are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val raw = (\n",
    "    spark.read.option(\"inferSchema\", \"true\")\n",
    "    .json(\"../data/winemag-data-130k-v2.json.gz\")\n",
    ")\n",
    "\n",
    "raw.cache\n",
    "raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean, dedupe and sanitize\n",
    "\n",
    "For make the model training as effective as possible later, an important step is to clean, dedup and sanitize the data:\n",
    "\n",
    "* Select just the variety and description and apply trim and lowercase functions to the DF _variety_ and _description_ columns.\n",
    "* Drop any duplicate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var df = (\n",
    "    raw\n",
    "    .select(trim(lower($\"variety\")) as \"variety\", trim(lower($\"description\")) as \"description\")\n",
    "    .orderBy($\"variety\")\n",
    ").cache\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.dropDuplicates(Seq(\"description\"))).cache\n",
    "df.orderBy($\"variety\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.filter($\"variety\".isNotNull)).cache\n",
    "df.orderBy($\"variety\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df        \n",
    "    .filter(not($\"variety\".contains(\"blend\")))\n",
    "    .filter(not($\"variety\".contains(\"red\")))\n",
    "    .filter(not($\"variety\".contains(\"white\")))\n",
    "    .filter(not($\"variety\".contains(\"rose\")))\n",
    ").cache\n",
    "df.orderBy($\"variety\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the variety name and format the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sanitizedDf = (df    \n",
    "    .select($\"variety\", regexp_replace($\"description\", $\"variety\", lit(\"\")) as \"description\")\n",
    "    .select($\"variety\", regexp_replace($\"description\", \"[^\\\\p{L}\\\\p{Nd}[0-9]+]+\", \" \") as \"description\")\n",
    "    .select($\"variety\", trim(lower($\"description\")) as \"description\")\n",
    "    .orderBy($\"variety\")\n",
    ").cache\n",
    "\n",
    "sanitizedDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://i.kinja-img.com/gawker-media/image/upload/s--aUvhMJJ8--/bam2tzwtb5cdfwctsdkc.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set minimum number of reviews per variety for the data to be included in training the model\n",
    "\n",
    "This will ensure that we have a reasonable set of reviews per variety upon which to train the model, otherwise generating a model will not be as effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val minimumReviewsPerVariety = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val varietyCounts = (\n",
    "    raw\n",
    "    .groupBy($\"variety\")\n",
    "    .agg(\n",
    "        count(\"variety\") as \"count\",\n",
    "        min(\"price\") as \"min_price\",\n",
    "        max(\"price\") as \"max_price\",\n",
    "        round(mean(\"price\")) as \"mean_price\",\n",
    "        round(stddev(\"price\")) as \"price_stddev\",\n",
    "        round(mean(\"points\")) as \"points\",\n",
    "        round(stddev(\"points\")) as \"potins_stddev\"\n",
    "    )\n",
    "    .where(\"count > \" + minimumReviewsPerVariety)\n",
    "    .orderBy($\"count\".desc)\n",
    ")\n",
    "\n",
    "varietyCounts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%brunel\n",
    "\n",
    "data('varietyCounts')\n",
    "bar \n",
    "x(variety)\n",
    "y(mean_price)\n",
    "color(variety)\n",
    "sort(mean_price)\n",
    "style('* {font-size: 7pt}') :: width=1000, height=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%brunel \n",
    "    data('varietyCounts') \n",
    "    bubble color(count) \n",
    "    size(count) \n",
    "    sort(count) \n",
    "    label(variety, count) \n",
    "    tooltip(#all) \n",
    "    style('* {font-size: 7pt}') :: width=1000, height=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val varietyCounts2 = (\n",
    "    raw\n",
    "    .groupBy($\"variety\", $\"country\")\n",
    "    .count()\n",
    "    .where(\"count > \" + minimumReviewsPerVariety)\n",
    ")\n",
    "\n",
    "varietyCounts2.orderBy($\"count\".desc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%brunel\n",
    "data('varietyCounts2') \n",
    "bar \n",
    "x(country) \n",
    "sort(count) \n",
    "y(count) \n",
    "color(country) \n",
    "style('* {font-size: 7pt}') :: width=1000, height=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%brunel \n",
    "    data('varietyCounts2') \n",
    "    chord x(country) y(variety) \n",
    "    color(count) \n",
    "    size(count) \n",
    "    sort(count) \n",
    "    label(variety) \n",
    "    tooltip(#all) :: width=1000, height=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the varieties with > 2000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts = (sanitizedDf\n",
    "    .groupBy($\"variety\")\n",
    "    .agg(count($\"variety\") as \"count\")\n",
    "    .where(\"count > \" + minimumReviewsPerVariety)\n",
    "    .join(sanitizedDf, Seq(\"variety\"))\n",
    "    .orderBy(\"variety\")\n",
    "    .select($\"variety\", $\"description\", $\"count\"))\n",
    "\n",
    "counts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create tokenizer to reove any outstanding variety names from the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val varietySplits = (\n",
    "    new Tokenizer()             \n",
    "    .setInputCol(\"variety\")\n",
    "    .setOutputCol(\"variety_splits\")\n",
    "    .transform(counts.select(\"variety\").distinct())\n",
    "    .select(\"variety_splits\")\n",
    "    .collect()\n",
    "    .map(_.toSeq.asInstanceOf[WrappedArray[WrappedArray[String]]])\n",
    "    .flatMap(_.toSeq)\n",
    "    .flatMap(_.toSeq)\n",
    "    .toList\n",
    ")\n",
    "\n",
    "val tokenizer = (\n",
    "    new Tokenizer()\n",
    "    .setInputCol(\"description\")\n",
    "    .setOutputCol(\"words\")\n",
    ")\n",
    "\n",
    "val stopWordsRemover = (\n",
    "    new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"filteredWords\")\n",
    ")\n",
    "\n",
    "val stopwords = stopWordsRemover.setStopWords((varietySplits:::stopWordsRemover.getStopWords.toList).toSet.toArray)\n",
    "\n",
    "println(stopwords.getStopWords.mkString(\",\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Spark ML Pipeline\n",
    "\n",
    "![alt text](https://spark.apache.org/docs/2.3.1/img/ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [CountVectorizer](https://spark.apache.org/docs/2.3.1/ml-features.html#countvectorizer)\n",
    "\n",
    "CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts. When an a-priori dictionary is not available, CountVectorizer can be used as an Estimator to extract the vocabulary, and generates a CountVectorizerModel. The model produces sparse representations for the documents over the vocabulary, which can then be passed to other algorithms like LDA.\n",
    "\n",
    "During the fitting process, CountVectorizer will select the top vocabSize words ordered by term frequency across the corpus. An optional parameter minDF also affects the fitting process by specifying the minimum number (or fraction if < 1.0) of documents a term must appear in to be included in the vocabulary. Another optional binary toggle parameter controls the output vector. If set to true all nonzero counts are set to 1. This is especially useful for discrete probabilistic models that model binary, rather than integer, counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n",
    "\n",
    "val countVectorizer = (\n",
    "    new CountVectorizer()\n",
    "    .setInputCol(stopWordsRemover.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [StringIndexer](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer)\n",
    "\n",
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in \\[0, numLabels), ordered by label frequencies, so the most frequent label gets index 0. The unseen labels will be put at index numLabels if user chooses to keep them. If the input column is numeric, we cast it to string and index the string values. When downstream pipeline components such as Estimator or Transformer make use of this string-indexed label, you must set the input column of the component to this string-indexed column name. In many cases, you can set the input column with setInputCol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val indexer = (\n",
    "    new StringIndexer()\n",
    "    .setInputCol(\"variety\")\n",
    "    .setOutputCol(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train a NaiveBayes classifier on the count vectorised features\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/0QOII.png)\n",
    "\n",
    "A really important yet subtle point here is that in order to ensure the test and training sets are always the same, when the dataset is split we use a set seed for the PRG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val Array(trainingData, testData) = (\n",
    "    indexer           \n",
    "    .fit(counts)\n",
    "    .transform(counts)\n",
    "    .randomSplit(Array(0.7, 0.3), 42L)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bind the stages to the pipline and train against the _training_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val naiveBayes = new NaiveBayes()\n",
    "\n",
    "val pipeline = (\n",
    "    new Pipeline()\n",
    "    .setStages(\n",
    "        Array(\n",
    "            tokenizer, \n",
    "            stopWordsRemover, \n",
    "            countVectorizer,\n",
    "            naiveBayes\n",
    "        )\n",
    "    )\n",
    ")\n",
    "            \n",
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run predictions on the test data to check how well the model performs on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(testData)\n",
    "\n",
    "val evaluator = (new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\"))\n",
    "\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(\"Test set accuracy should be 0.7738423373759648 = \" + (accuracy == 0.7738423373759648))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
